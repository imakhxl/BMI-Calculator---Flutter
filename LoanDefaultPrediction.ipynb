{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOMQaFP4b+/fXcwsWTnFl5N",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/imakhxl/BMI-Calculator---Flutter/blob/master/LoanDefaultPrediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xOET9Eb0S6ap"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Load data sets\n",
        "train_data = pd.read_csv(\"train.csv\")\n",
        "test_data = pd.read_csv(\"test.csv\")\n",
        "\n",
        "# Data exploration and cleaning (replace with your specific exploration code)\n",
        "# ... Explore data distributions, identify missing values, and handle them\n",
        "\n",
        "# Feature engineering (replace with your specific feature engineering code)\n",
        "# ... Create new features, encode categorical features (e.g., one-hot encoding)\n",
        "\n",
        "# Separate features and target variable\n",
        "train_features = train_data.drop(columns=[\"LoanID\", \"Default\"])\n",
        "train_target = train_data[\"Default\"]\n",
        "\n",
        "# Split training data for model training and validation (optional)\n",
        "# You can use train_test_split to split the training data for model training and validation\n",
        "# This helps prevent overfitting and evaluate model performance on unseen data within the training set.\n",
        "# X_train, X_val, y_train, y_val = train_test_split(train_features, train_target, test_size=0.2, random_state=42)\n",
        "\n",
        "# Feature scaling (optional)\n",
        "# You can use techniques like StandardScaler or MinMaxScaler to normalize feature values\n",
        "# This can improve the performance of some machine learning algorithms.\n",
        "# ... Implement feature scaling if needed\n",
        "\n",
        "# Model selection and training\n",
        "# Choose an appropriate model for binary classification (e.g., Logistic Regression, Random Forest)\n",
        "model = LogisticRegression()  # Replace with your chosen model\n",
        "model.fit(train_features, train_target)\n",
        "\n",
        "# Model evaluation (optional on validation set if split earlier)\n",
        "# ... Evaluate model performance on the validation set using metrics like accuracy, precision, recall, and F1-score\n",
        "\n",
        "# Prediction on test data\n",
        "test_features = test_data.drop(columns=[\"LoanID\"])\n",
        "predicted_probabilities = model.predict_proba(test_features)[:, 1]  # Select probability of default class\n",
        "\n",
        "# Create submission dataframe\n",
        "prediction_df = pd.DataFrame({\"LoanID\": test_data[\"LoanID\"], \"predicted_probability\": predicted_probabilities})\n",
        "\n",
        "# Save the submission dataframe\n",
        "prediction_df.to_csv(\"submission.csv\", index=False)\n",
        "\n",
        "print(\"Prediction completed! Submission file generated as 'submission.csv'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Miscellaneous"
      ],
      "metadata": {
        "id": "7aFCFQAtTy57"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "data_descriptions = pd.read_csv('data_descriptions.csv')\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "data_descriptions"
      ],
      "metadata": {
        "id": "BF08MEISTZlq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data packages\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Machine Learning / Classification packages\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.dummy import DummyClassifier\n",
        "\n",
        "# Visualization Packages\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "RxHCeqfKT4J2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.compose import ColumnTransformer"
      ],
      "metadata": {
        "id": "d89ZUJQWT4G9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.read_csv(\"train.csv\")\n",
        "print('train_df Shape:', train_df.shape)\n",
        "train_df.head()"
      ],
      "metadata": {
        "id": "L267bcpiT4DV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df = pd.read_csv(\"test.csv\")\n",
        "print('test_df Shape:', test_df.shape)\n",
        "test_df.head()"
      ],
      "metadata": {
        "id": "4lE0A8XIT4B7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def impute_missing_values(df):\n",
        "  df[\"Income\"].fillna(df[\"Income\"].mean(), inplace=True)\n",
        "  df[\"Education\"].fillna(df[\"Education\"].mode()[0], inplace=True)\n",
        "  return df\n",
        "\n",
        "cleaned_train_df = impute_missing_values(train_df.copy())\n",
        "cleaned_test_df = impute_missing_values(test_df.copy())\n",
        "\n",
        "def encode_categorical_features(df):\n",
        "\n",
        "  le = LabelEncoder()\n",
        "  ohe = OneHotEncoder(sparse=False)\n",
        "\n",
        "  categorical_features = [\"MaritalStatus\", \"EmploymentType\"]\n",
        "\n",
        "def encode_categorical_features(df):\n",
        "  le = LabelEncoder()\n",
        "  ohe = OneHotEncoder(sparse=False)\n",
        "\n",
        "  categorical_features = [\"MaritalStatus\", \"EmploymentType\"]\n",
        "\n",
        "  for col in categorical_features:\n",
        "      if col in df.columns:\n",
        "          if col == \"MaritalStatus\":\n",
        "              marital_features = ohe.fit_transform(df[[col]])\n",
        "              marital_df = pd.DataFrame(marital_features, columns=ohe.get_feature_names([col]))\n",
        "              df = pd.concat([df, marital_df], axis=1)\n",
        "              df.drop(col, axis=1, inplace=True)\n",
        "          else:\n",
        "              df[col] = le.fit_transform(df[col])\n",
        "\n",
        "  if \"EmploymentType\" in df.columns:\n",
        "      df[\"EmploymentType\"].replace(\"No\", np.NAN, inplace=True)\n",
        "\n",
        "  if \"MaritalStatus\" in df.columns:\n",
        "      df[\"MaritalStatus\"].replace(\"No\", np.NAN, inplace=True)\n",
        "\n",
        "  if \"HasMortgage\" in df.columns:\n",
        "      df[\"HasMortgage\"].replace(\"No\", np.NAN, inplace=True)\n",
        "\n",
        "  if \"HasDependents\" in df.columns:\n",
        "      df[\"HasDependents\"].replace(\"No\", np.NAN, inplace=True)\n",
        "\n",
        "  return df\n",
        "\n",
        "\n",
        "cleaned_train_df = encode_categorical_features(cleaned_train_df)\n",
        "cleaned_test_df = encode_categorical_features(cleaned_test_df)\n",
        "\n",
        "\n",
        "print(\"Sample of cleaned training data:\")\n",
        "print(cleaned_train_df.head())\n",
        "\n",
        "print(\"Sample of cleaned testing data:\")\n",
        "print(cleaned_test_df.head())\n",
        "\n"
      ],
      "metadata": {
        "id": "4LRxDfC5T3-2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = cleaned_train_df\n",
        "test_df = cleaned_test_df\n",
        "\n",
        "train_features = train_df.drop(columns=[\"LoanID\", \"Default\"])\n",
        "train_target = train_df[\"Default\"]"
      ],
      "metadata": {
        "id": "XAV_t6bwT39c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(train_features, train_target, test_size=0.2, random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "\n",
        "categorical_features = [col for col in train_features.columns if train_features[col].dtype == 'object']\n",
        "numerical_features = [col for col in train_features.columns if col not in categorical_features]\n",
        "transformer = ColumnTransformer([(\"scaler\", scaler, numerical_features)], remainder='passthrough')\n",
        "train_features_scaled = transformer.fit_transform(X_train)\n",
        "val_features_scaled = transformer.transform(X_val)\n",
        "test_features_scaled = transformer.transform(test_df.drop(columns=[\"LoanID\"]))"
      ],
      "metadata": {
        "id": "FsgOPfjDT36P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = LogisticRegression()\n",
        "model.fit(train_features_scaled, y_train)\n",
        "\n",
        "predicted_probabilities = model.predict_proba(test_features_scaled)[:, 1]"
      ],
      "metadata": {
        "id": "--z9cKibT35A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(prediction_df.head())"
      ],
      "metadata": {
        "id": "sJjweP3ZT31o"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}